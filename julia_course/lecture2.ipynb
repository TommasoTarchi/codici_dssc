{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Short Tour of Julia\n",
    "\n",
    "In the first part of the lecture we will see:\n",
    "\n",
    "- Managing data with ```DataFrames.jl```\n",
    "- Neural networks with ```Flux.jl```\n",
    "- Benchmarking with ```BenchmarkTools.jl```\n",
    "- Plotting: ```Plots.jl``` and others libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m  Activating\u001b[22m\u001b[39m new project at `~/codici/codici_dssc/julia_course`\n"
     ]
    }
   ],
   "source": [
    "using Pkg\n",
    "Pkg.activate(\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ArgumentError",
     "evalue": "ArgumentError: Package DataFrames not found in current path.\n- Run `import Pkg; Pkg.add(\"DataFrames\")` to install the DataFrames package.",
     "output_type": "error",
     "traceback": [
      "ArgumentError: Package DataFrames not found in current path.\n",
      "- Run `import Pkg; Pkg.add(\"DataFrames\")` to install the DataFrames package.\n",
      "\n",
      "Stacktrace:\n",
      "  [1] macro expansion\n",
      "    @ ./loading.jl:1163 [inlined]\n",
      "  [2] macro expansion\n",
      "    @ ./lock.jl:223 [inlined]\n",
      "  [3] require(into::Module, mod::Symbol)\n",
      "    @ Base ./loading.jl:1144\n",
      "  [4] eval\n",
      "    @ ./boot.jl:368 [inlined]\n",
      "  [5] include_string(mapexpr::typeof(REPL.softscope), mod::Module, code::String, filename::String)\n",
      "    @ Base ./loading.jl:1428\n",
      "  [6] #invokelatest#2\n",
      "    @ ./essentials.jl:729 [inlined]\n",
      "  [7] invokelatest\n",
      "    @ ./essentials.jl:726 [inlined]\n",
      "  [8] (::VSCodeServer.var\"#198#199\"{VSCodeServer.NotebookRunCellArguments, String})()\n",
      "    @ VSCodeServer ~/.vscode/extensions/julialang.language-julia-1.38.2/scripts/packages/VSCodeServer/src/serve_notebook.jl:19\n",
      "  [9] withpath(f::VSCodeServer.var\"#198#199\"{VSCodeServer.NotebookRunCellArguments, String}, path::String)\n",
      "    @ VSCodeServer ~/.vscode/extensions/julialang.language-julia-1.38.2/scripts/packages/VSCodeServer/src/repl.jl:249\n",
      " [10] notebook_runcell_request(conn::VSCodeServer.JSONRPC.JSONRPCEndpoint{Base.PipeEndpoint, Base.PipeEndpoint}, params::VSCodeServer.NotebookRunCellArguments)\n",
      "    @ VSCodeServer ~/.vscode/extensions/julialang.language-julia-1.38.2/scripts/packages/VSCodeServer/src/serve_notebook.jl:13\n",
      " [11] dispatch_msg(x::VSCodeServer.JSONRPC.JSONRPCEndpoint{Base.PipeEndpoint, Base.PipeEndpoint}, dispatcher::VSCodeServer.JSONRPC.MsgDispatcher, msg::Dict{String, Any})\n",
      "    @ VSCodeServer.JSONRPC ~/.vscode/extensions/julialang.language-julia-1.38.2/scripts/packages/JSONRPC/src/typed.jl:67\n",
      " [12] serve_notebook(pipename::String, outputchannel_logger::Base.CoreLogging.SimpleLogger; crashreporting_pipename::String)\n",
      "    @ VSCodeServer ~/.vscode/extensions/julialang.language-julia-1.38.2/scripts/packages/VSCodeServer/src/serve_notebook.jl:139\n",
      " [13] top-level scope\n",
      "    @ ~/.vscode/extensions/julialang.language-julia-1.38.2/scripts/notebook/notebook.jl:32"
     ]
    }
   ],
   "source": [
    "using DataFrames\n",
    "using Plots\n",
    "using Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Managing Data with DataFrames\n",
    "\n",
    "DataFrames on Julia are similar to dataframes in R and Pandas dataframes in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us start by creating an _empty_ dataframe. We will add/load new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An empty dataframe is quite boring, let us generate some data:\n",
    "- $x$ positions from $1$ to $10$\n",
    "- a first random $y$ coordinate (uniform in $[0,1)$)\n",
    "- a second random $y$ coordinate ($N(0,1)$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = hcat(collect(1:10), rand(10, 1), randn(10, 1))\n",
    "df = DataFrame(data, :auto)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can rename the columns of the dataframe by passing a vector of Strings of Symbols to the ```rename!``` function (notice the ```!```)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = [:x, :y₁, :y₂]\n",
    "rename!(df, names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could have also added the names during the creation of the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = DataFrame(data, [:x, :y₁, :y₂])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can access the different columns of the dataframe by name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.y₁ # An array of Float64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[:, :x] # This looks more similar to array/dictionary access "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.\"x\" # We can even use strings..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can add a new column by simply assigning a vector of suitable length ($10$ elements in this case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.x₂ = 10*rand(10)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We might want to rename and reorder the columns.\n",
    "\n",
    "We can do this via the ```rename``` and ```select!``` functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rename!(df, :x => :x₁)\n",
    "select!(df, r\"x\", :) # group all columns matching the regexp \"x\" before all the rest (:)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us get some statistics on this data via the ```describe``` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "describe(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can select which statstics to get via additional arguments to the ```describe``` function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we need a matrix, instead of a dataframe, simply using the ```Matrix()``` constructor works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Matrix(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before moving on, let us start with our first scatter plot.\n",
    "\n",
    "- ```scatter``` creates a new plot\n",
    "- ```scatter!``` adds to the existing plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter(df.x₁, df.y₁, label=\"data 1\")\n",
    "scatter!(df.x₂, df.y₂, label=\"data 2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A few notes on plotting in Julia\n",
    "\n",
    "There are multiple packages that can be used for plotting in Julia:\n",
    "\n",
    "- ```Plots.jl```: the \"main\" Julia plotting library with multiple backends (including in JavaScript)\n",
    "- ```PyPlots.jl```: wrapper for Python's matplotlib\n",
    "- ```Gadfly.jl```: promising package, inspired by ggplot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manipulation of Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = DataFrame()\n",
    "\n",
    "for i ∈ 1:10^5\n",
    "    elem::Vector{Float64} = []\n",
    "    while sum(elem) ≤ 1\n",
    "        push!(elem, rand())\n",
    "    end\n",
    "    push!(df, (id = i, length = length(elem), elements = elem))\n",
    "end\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maybe we want to add the sum of all the elements in each list as an additional column in out dataframe.\n",
    "\n",
    "Notice that ```ByRow``` indicates that the function is applied to each row of the column, not to the entire column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform!(df, :elements => ByRow(sum))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```elements_sum``` is not a good name. Let us delete the column and create it again with a different name (without using ```rename!``` which would be better)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "select!(df, :id, :length, :elements)\n",
    "transform!(df, :elements => ByRow(sum) => :sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us find the average length of the list of elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean(df.length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is possible to prove that the expecte value is the constant $e$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MathConstants.e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could have used ```combine``` to _combine_ all elements of a column in a single value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combine(df, :length => mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us explore how we can group the different rows of the dataframe using the ```groupby``` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_df = groupby(df, :length, sort=true) |> x -> combine(x, :length => length => :num_elems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histogram(df.length, yaxis = :log, bar_width = 0.75, title = \"number of sequences\", key=false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Neural Network\n",
    "\n",
    "We are going to build a simple neural network from scratch, then we are going to use the facilities provided by ```Flux.jl``` to help us build and train neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Flux"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automatic gradient computation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us define a function of which we want to compute the derivative:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "f (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "f(x) = 3x^3 + 2x^2 + 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compute the derivative by using the ```gradient``` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "derivative_f (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "derivative_f(x) = gradient(f, x)[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us plot both $f$ and its derivative:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "UndefVarError",
     "evalue": "UndefVarError: plot not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: plot not defined\n",
      "\n",
      "Stacktrace:\n",
      " [1] top-level scope\n",
      "   @ ~/codici/codici_dssc/julia_course/lecture2.ipynb:3"
     ]
    }
   ],
   "source": [
    "x_vals = -5:0.01:5\n",
    "\n",
    "plot(x_vals, f.(x_vals), label=\"f(x)\")\n",
    "plot!(x_vals, derivative_f.(x_vals), label=\"f'(x)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we expect $9x^2 + 4x$ as a derivative and a good automatic differentiation engine will actually write the code corresponding to it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@code_llvm derivative_f(3.0) # we expect 9x^2 + 4x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For an introduction to automatic diffentiation the [wikipedia page](https://en.wikipedia.org/wiki/Automatic_differentiation) provides a good overview.\n",
    "\n",
    "For one of the automatic differentiation framework in Julia that is used in Flux see [Zigote.jl](https://github.com/FluxML/Zygote.jl) and the paper describing how automatic differentiation is performed on [arXiv](https://arxiv.org/abs/1810.07951)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Networks from scratch\n",
    "\n",
    "For a general introduction to machine learning a quick read is [The hundred-page machine learning book](http://themlbook.com/wiki/doku.php) where all chapter are available online. For a more in-depth course on neural networks and deep learning, we refer to the [Deep Learning course](https://atcold.github.io/pytorch-Deep-Learning/) by Yann LeCun and Alfredo Canziani."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us build a simple fully connected layer (i.e., a simple linear function) with two inputs and one output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = rand(1, 2) .- 0.5;\n",
    "b = rand(1) .- 0.5;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of this linear function is $Wx + b$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_layer(x) = W*x .+ b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The error between the expected outputs $y = (y_1, \\ldots, y_n)$ and the outputs $\\hat{y} = (\\hat{y}_1, \\ldots, \\hat{y}_n)$ given by the layer is $\\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat{y}_i)^2$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function loss(x, y)\n",
    "    ŷ = simple_layer(x)\n",
    "    mean((y .- ŷ).^2)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compute the gradient by using the ```gradient``` function made available by Flux, and we can decide to derive whith respect to what parameters by using ```Flux.params```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_simple(x, y) = gradient(() -> loss(x,y), Flux.params(W, b))\n",
    "\n",
    "println(d_simple([2, 3], 4)[W])\n",
    "println(d_simple([2, 3], 4)[b])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How can we train this simple neural network/linear function? by using gradient descent. Notice that here we use ```global``` like in Python to modify a variable in the global scope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function train!(X, Y; η=0.1)\n",
    "    grad = d_simple(X,Y)\n",
    "    W̃ = grad[W]\n",
    "    b̃ = grad[b]\n",
    "    global W = W - η*W̃\n",
    "    global b = b - η*b̃\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us generate a very simple training set as $100$ random point where the target value is actually a linear function of first component plus a gaussian noise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_X = rand(2, 100)\n",
    "simple_y = [10*i + randn()*0.05 + 3 for i ∈ simple_X[1,:]];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us visualize the data in 3D:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function show_data()\n",
    "    scatter3d(simple_X[1,:], simple_X[2,:], simple_y, label=\"target\")\n",
    "    simple_ŷ = simple_layer(simple_X)\n",
    "    scatter3d!(simple_X[1,:], simple_X[2,:], reshape(simple_ŷ, (100,)), label=\"predicted\")\n",
    "end\n",
    "\n",
    "show_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now train for a few epochs the network, printing the loss before and after the training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "println(\"Loss before training : $(loss(simple_X, simple_y))\")\n",
    "for _ in 1:1000\n",
    "    train!(simple_X, simple_y)\n",
    "end\n",
    "println(\"Loss after training: $(loss(simple_X, simple_y))\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us download the MNIST dataset, which contains $60,000$ images of handwritten digits as $28x28$ greyscale images.\n",
    "\n",
    "You can find it within the ```MLDatasets``` package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using MLDatasets\n",
    "images = MLDatasets.MNIST().features\n",
    "labels = MLDatasets.MNIST().targets;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us see one of the images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images[:,:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first image has label: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some standard preprocessing:\n",
    "\n",
    "- encoding the labels as one-hot vectors of $10$ elements\n",
    "- For this example we will use only $1,000$ images instead of $60,000$\n",
    "- change the type of the images as arrays of ```Float64``` and the shape of the input as $28 \\times 28 \\times \\textit{ num channels } \\times \\textit{ num samples}$\n",
    "- prepare the minibatches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_images = 1000\n",
    "\n",
    "Y = Flux.onehotbatch(labels[1:n_images], 0:9);\n",
    "\n",
    "X = Float32.(reshape(images[:,:,1:n_images], (28, 28, 1, n_images)))\n",
    "\n",
    "batches = Flux.Data.DataLoader((X, Y), batchsize=32);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now build our model as a convolutional neural network with:\n",
    "\n",
    "- convolutional layers, with a $3x3$ kernel, a padding of $1$ in all directions and with _input channels_ ```=>``` _output channels_\n",
    "- max pooling layers\n",
    "- a dense layer with $288$ inputs and $10$ outpus followed by a softmax layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Chain(\n",
    "    Conv((3,3), 1 => 16, relu, pad=(1,1)),\n",
    "    MaxPool((2,2)),\n",
    "    Conv((3,3), 16 => 32, relu, pad=(1,1)),\n",
    "    MaxPool((2,2)),\n",
    "    Conv((3,3), 32 => 32, relu, pad=(1,1)),\n",
    "    MaxPool((2,2)),\n",
    "    Flux.flatten,\n",
    "    Dense(288, 10, identity),\n",
    "    softmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can define the loss as the _crossentropy_ loss. Notice that the model is now included in the definition of the loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function loss(x, y)\n",
    "    ŷ = model(x)\n",
    "    Flux.Losses.crossentropy(y, ŷ)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to the loss function, we are interested in the accuracy of the prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function accuracy(x, y)\n",
    "    mean(Flux.onecold(model(x)) .== Flux.onecold(y))\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We decide which optimizer to use (e.g., ADAM, ADAGrad, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = Flux.ADAM()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also define a callback function to be called at most once every ```n_seconds``` during training to print the current value of the loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_seconds = 5\n",
    "cb = Flux.throttle(() -> println(\"Current loss: $(loss(X, Y))\"), n_seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How good is our untrained network?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy(X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now train the network. Notice that we also have a macro ```Flux.@epochs num_epochs code``` available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i ∈ 1:10\n",
    "    println(\"Epoch $i\")\n",
    "    Flux.train!(loss, Flux.params(model), batches, optim, cb=cb)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that our accuracy has improved (but not by a lot, we had a very short learning phase):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy(X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmarking\n",
    "\n",
    "We have seen a few ways of exploring how much time a certain operation requires in Julia, using the ```@time``` or the ```@timed``` macros.\n",
    "\n",
    "Let us start by benchmarking this function ```my_sum``` with a $10^6$ vector of random elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function my_sum(v)\n",
    "    s = 0.0 # zero(eltype(v)) # would be better since it will use the \"correct\" zero\n",
    "    for x ∈ v\n",
    "        s += x\n",
    "    end\n",
    "    s\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_vec = rand(10^6);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us time the function using the ```@time``` macro:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@time my_sum(rand_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@time my_sum(rand_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since Julia is JIT-compiled the first execution includes the compilation and might not be representative of the successive execution. Furthermore, we need more than one execution to get some significant result!\n",
    "\n",
    "We can use the ```BenchmarkTools.jl``` package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using BenchmarkTools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have the ```@benchmark``` macro that executes the code multiple times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@benchmark my_sum($rand_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that there is still some overhead, since we are calling python code from Julia.\n",
    "\n",
    "But what about the Julia native sum implementation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@benchmark sum($rand_vec)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## And now a small break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter(randn(3,2000) .+ [-3,0,3], randn(3, 2000) .+ [-3,2,-3], \n",
    "        c=palette(:default)[2:4], key=:none, grid=false, showaxis=false,\n",
    "        ticks=false, size=(600,600), markerstrokewidth=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Short Tour of Julia\n",
    "\n",
    "In this part of the lecture we will explore:\n",
    "\n",
    "- Differential equations with ```DifferentialEquations.jl```\n",
    "- Probabilistic programming with ```Turing.jl```\n",
    "- Distributed computation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Differential Equations\n",
    "\n",
    "The ```DifferentialEquations.jl``` package makes easy to define and solve multiple kinds of differential equations, with multiple solvers available depending on the kind of equation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using DifferentialEquations\n",
    "using Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us define a simple ordinary differential equation \n",
    "$$ \\frac{du}{dt} = \\alpha u $$\n",
    "where $\\alpha$ is a parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "α = 1.02\n",
    "f(u, p, t) = α*u"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default in the definition of the ODE there are:\n",
    "- The variable $u$.\n",
    "- A collection of parameters to the ODE (we will show how to use them shortly).\n",
    "- The time $t$.\n",
    "\n",
    "We now need to define the initial state $u_0$ and the time interval for which we want to solve our ODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u0 = 0.03\n",
    "tspan = (0.0, 1.0);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now define an ODE problem, which is entirely determined by the ODE, the initial conditions, and the timespan:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob = ODEProblem(f, u0, tspan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To obtain a solution we can simply call ```solve```. As additional arguments we can specify:\n",
    "\n",
    "- the solver to use (see the [documentation](https://diffeq.sciml.ai/v6.16/solvers/ode_solve/#ode_solve)).\n",
    "- relative and absolute tolerances (keyword arguments ```reltol``` and ```abstol```, respectively).\n",
    "- we might also only give a _hint_ to select the solver with, for example, ```alg_hints=[:stiff]```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sol = solve(prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now plot the solution (and the analytical solution for comparison):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(sol, lw=3, xaxis=\"Time (t)\", yaxis=\"u(t)\", legend=false)\n",
    "plot!(0:0.01:1, t->u0*exp(α*t), lw=3, ls=:dash)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the structure ```sol``` can be used as a function that, for values that were not computed, provides an interpolation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[sol(i) for i ∈ 0:0.1:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The solution structure also contains information about the pair $(t,u)$ that were computed during the solution process in ```sol.t``` and ```sol.u```:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "println(\"t = $(sol.t)\\nu = $(sol.u)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters\n",
    "\n",
    "We had one argument of $f$ that was not used: ```p```.\n",
    "\n",
    "```p``` can be an strucure of any type containing the parameters that we want to use in the differential equation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g(u, p, t) = p * u"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The value of the parameters can be passed as an argument of the ODE problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob = ODEProblem(g, u0, tspan, 1.03)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everyting continues to work as before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sol = solve(prob, Tsit5(), reltol=1e-8, abstol=1e-8)\n",
    "plot(sol,lw=3, xaxis=\"Time (t)\", yaxis=\"u(t)\", legend=false)\n",
    "plot!(sol.t, t->u0*exp(1.03*t), lw=3, ls=:dash)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In-place\n",
    "\n",
    "Until now we had ```In-place: false``` for all our ODE problem. This means that there is a new allocation every time we need to compute a new value of ```f```. We can avoid the allocation by using an ```in-place``` definition where the value is returned by modifing one of the arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function f!(du, u, p, t)\n",
    "    du[1] = p * u[1]\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_inplace = ODEProblem(f!, [u0], tspan, α)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two things to notice:\n",
    "\n",
    "- One additional argument (the first argument) to $f!$, which is the value to be modified.\n",
    "- The value of ```In-place``` when the ```ODEProblem``` is defined is now ```true```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sol_inplace = solve(prob_inplace)\n",
    "plot(sol_inplace, lw=3, xaxis=\"Time (t)\", yaxis=\"u(t)\", legend=false)\n",
    "plot!(0:0.01:1, t->u0*exp(α*t), lw=3, ls=:dash)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lotka-Volterra Equations\n",
    "\n",
    "The Lotka-Volterra equations are a model of population dynamics where there are two species, one acting as prey and one as predator:\n",
    "\n",
    "$$\n",
    "\\frac{dx}{dt} = \\alpha x - \\beta xy \\\\\n",
    "\\frac{dy}{dt} = \\delta xy - \\gamma y\n",
    "$$\n",
    "\n",
    "Here $x$ is the size of the prey population and $y$ the size of the predator population and:\n",
    "\n",
    "- $\\alpha$ is the rate at which the preys increase in number.\n",
    "- $\\beta$ is the rate at which the preys are killed by the predators.\n",
    "- $\\gamma$ is the rate at which the predators die or leave the territory.\n",
    "- $\\delta$ is the rate at which the population of predators increases, that depends on how many preys they are able to catch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now define an (in-place) function for the Lotka-Volterra equations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function lotka_volterra!(du, u, p, t)\n",
    "    α, β, γ, δ = p\n",
    "    du[1] = α*u[1] - β*u[1]*u[2]\n",
    "    du[2] = δ*u[1]*u[2] - γ*u[2]\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now model a populations starting at the same size, with a default set of parameters (recall that we can use named tuples to improve readability):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u0 = [10, 10]\n",
    "p = (α = 1.1, β = 0.4, γ = 0.4, δ = 0.1)\n",
    "tspan = (0.0, 100.0)\n",
    "prob_lv = ODEProblem(lotka_volterra!, u0, tspan, p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding the evolution in time of the two populations can still be obtained by simply calling the ```solve``` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sol_lv = solve(prob_lv);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(sol_lv.t, [u[1] for u ∈ sol_lv.u], label=\"prey\")\n",
    "plot!(sol_lv.t, [u[2] for u ∈ sol_lv.u], label=\"predator\")\n",
    "xlabel!(\"time\")\n",
    "ylabel!(\"population size\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Others Types of Differential Equations\n",
    "\n",
    "The library ```DifferentialEquations.jl``` can also manage many kinds of differential equations (from the [documentation](https://diffeq.sciml.ai/v6.16/)):\n",
    "\n",
    "- Discrete equations (function maps, discrete stochastic (Gillespie/Markov) simulations)\n",
    "- Ordinary differential equations (ODEs)\n",
    "- Split and Partitioned ODEs (Symplectic integrators, IMEX Methods)\n",
    "- Stochastic ordinary differential equations (SODEs or SDEs)\n",
    "- Random differential equations (RODEs or RDEs)\n",
    "- Differential algebraic equations (DAEs)\n",
    "- Delay differential equations (DDEs)\n",
    "- Stochastic delay differential equations (SDDEs)\n",
    "- Mixed discrete and continuous equations (Hybrid Equations, Jump Diffusions)\n",
    "- (Stochastic) partial differential equations ((S)PDEs) (with both finite difference and finite element methods)\n",
    "\n",
    "We will see as example the SIR model, where the jumps are discrete."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The SIR model\n",
    "\n",
    "The SIR model is used to simulate the effect of the diffusion of a disease inside a population. The states of each individual can be:\n",
    "\n",
    "- Susceptible\n",
    "- Infected\n",
    "- Recovered\n",
    "\n",
    "A susceptible person in contact with an infected one can become infected (with a certain rate $c_1$). An infected person transition to a recovered state with a certain rate $c_2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We import the ```Catalyst.jl``` module that allow us to write in a compact way a reaction network. I.e., we want to write that $s + i \\rightarrow 2 i$ with a certain rate $c_1$ and $i \\rightarrow r$ with a certain rate $c_2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Catalyst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Catalyst``` makes writing these reactions quite simple:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sir_model = @reaction_network begin\n",
    "    c1, s + i --> 2i\n",
    "    c2, i --> r\n",
    "    end c1 c2;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now set the initial conditions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = (1e-4, 0.01)\n",
    "tspan = (0.0, 300.0)\n",
    "u0 = [999, 1, 0] # 999 susceptible, 1 infected, 0 recovered\n",
    "prob_sir = DiscreteProblem(sir_model, u0, tspan, p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And find the dynamics of the disease:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_jump = JumpProblem(sir_model, prob_sir, Direct())\n",
    "sol_sir = solve(prob_jump, SSAStepper());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(sol_sir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Packages to check\n",
    "\n",
    "The following packages are related to ```DifferentialEquations.jl``` and can be interesting for specific applications:\n",
    "\n",
    "- ```DiffEqFlux.jl```, to create Neural ODE.\n",
    "- ```diffeqpy``` and ```diffeqr``` makes the solvers of ```DifferentialEquations.jl``` available to Python and R, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probabilistic Programming\n",
    "\n",
    "One of the main libraries in Julia for probabilistic programming is ```Turing.jl```.\n",
    "\n",
    "Programming works with a known model with known parameters to generate some data.\n",
    "\n",
    "Probabilistic programming is the case when the data and the model are known but we ignore the parameters.\n",
    "\n",
    "#### Some references\n",
    "\n",
    "[An Introduction to Probabilistic Programming](https://arxiv.org/abs/1809.10756)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us import ```Turing.jl``` plus a library of distributions, and a library to work with Makow chain Monte Carlo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Turing\n",
    "using Distributions\n",
    "using MCMCChains"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A model (prefixed with the macro ```@model```) is a function. The function can then be used to condition the model on the data.\n",
    "\n",
    "Here we model a coin flip (example taken from this [tutorial](https://turing.ml/dev/tutorials/0-introduction/)) where we do not know the probability $p$ of landing on head or tail.\n",
    "\n",
    "Here, we have $y_1, \\ldots, y_n$ samples that we know that theu will be distributed according to a Bernoulli distribution with parameter $p$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@model function coin(y)\n",
    "    p ~ Beta(1,1)\n",
    "   \n",
    "    for i ∈ 1:length(y)\n",
    "        y[i] ~ Bernoulli(p)\n",
    "    end\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now generate $100$ samples from a fair coin:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = rand(Bernoulli(0.5), 100);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then use the model to estimate the probability $p$ of landing on tail given that data that we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ϵ = 0.05\n",
    "τ = 10\n",
    "iterations = 1000\n",
    "chain = sample(coin(data), HMC(ϵ, τ), iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histogram(chain[:p], label=:none)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributed Computation\n",
    "\n",
    "We are going to explore how it is possible to easily distribute work across multiple processes (and, possibly, machines) in Julia.\n",
    "\n",
    "To see size of computations possible in Julia see the presentation [Celeste.jl: Petascale Computing in Julia](https://www.youtube.com/watch?v=uecdcADM3hY).\n",
    "\n",
    "First of all, we import the ```Distributed``` package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Distributed\n",
    "using BenchmarkTools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the function ```nprocs``` to see the number of active Julia processes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nprocs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additional worker processes can be added with the ```addproc``` function, which return an array of integer ids representing the newly created processes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "addprocs(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the number of Julia processes is now increased:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nprocs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will see a collection of macros, structures, and functions that are used for computation among multiple processes:\n",
    "\n",
    "- ```@everywhere``` to execute a block of code in all processes.\n",
    "- ```@spawnat``` to execute a function in a specific process.\n",
    "- ```Future``` and ```fetch``` the result \n",
    "- ```@sync```\n",
    "- ```@distributed```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our test case will be the following function to approximate $\\pi$ via a uniform sampling in $[0,1]^2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function approx_pi(n)\n",
    "    s = 0\n",
    "    for i ∈ 1:n\n",
    "        p = (rand(), rand())\n",
    "        s += p[1]^2 + p[2]^2 <= 1\n",
    "    end\n",
    "    4s/n\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "approx_pi(1_000_000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us benchmark it using ```BenchmarkTools.jl```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@btime approx_pi(10^7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now distribute the computation:\n",
    "\n",
    "- Define a function ```approx_pi_sum``` on every process\n",
    "- Split the $n$ points to sample in equal-length chunks and distribute it across processes with ```@spawnat```\n",
    "- Each computation returns a ```Future```\n",
    "- Wait for each computation to finish with ```@sync```\n",
    "- Fetch the results of the computation with ```fetch```\n",
    "- Finally compute this approximation of $\\pi$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@everywhere function approx_pi_sum(n)\n",
    "    s = 0\n",
    "    for i ∈ 1:n\n",
    "        p = (rand(), rand())\n",
    "        s += p[1]^2 + p[2]^2 <= 1\n",
    "    end\n",
    "    s\n",
    "end\n",
    "\n",
    "function compute_pi(n)\n",
    "    np = nprocs()\n",
    "    partial_sums = Vector(undef, np)\n",
    "    k = n ÷ np\n",
    "    @sync for i ∈ 1:np\n",
    "        partial_sums[i] = @spawnat i approx_pi_sum(k)\n",
    "    end\n",
    "    missings = n - np*k\n",
    "    s = approx_pi_sum(missings)\n",
    "    4*(sum(fetch.(partial_sums)) + s)/n\n",
    "end\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@btime compute_pi(10^7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this is so common, we can use the ```@distributed``` macros, that distribute the computation of a ```for``` cycle across all processes and reduce it with a given function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function distributed_pi(n)\n",
    "    sum = @distributed (+) for i ∈ 1:n\n",
    "            p = (rand(), rand())\n",
    "            p[1]^2 + p[2]^2 <= 1\n",
    "    end\n",
    "    4*sum/n\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distributed_pi(10^7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@btime distributed_pi(10^7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### See Also\n",
    "\n",
    "- ```MPI.jl``` - A Julia interface to MPI.\n",
    "- ```DistributedArrays.jl``` - Arrays that are stored on multiple processes possibly on different machines.\n",
    "- JuliaGPU and ```CUDA.jl``` to perform computations on GPU. Notice that ```Flux``` can use GPUs and also many other packages can work with them."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.8.3",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
