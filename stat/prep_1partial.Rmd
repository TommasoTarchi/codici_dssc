---
title: "esercizi per primo parziale"
author: "tommaso tarchi"
date: "2022-11-19"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## FSDS

### ex. 3.12

```{r}
library(MASS)

N <- 1000  # number of samples
n <- 10000  # size of the samples

data <- matrix(NA, N, n)
samps <- rep(NA, N)

for (i in 1:N) {
  data[i,] <- rnorm(n)
}
m <- apply(data, 1, mean)

hist.scott(m)
```
```{r}
mean(m)
sd(m); 1/sqrt(n)
```

### ex. 3.17

a) $Y$ is a Bernoulli random variable with probability $\pi=0.4$ (probability of being male).

b) It is again a Bernoulli, but this time with $\pi=\frac{32}{50}=0.64$.

c) Since the proportion can be seen seen as the the mean of the Bernoulli (M/F) sample, it is distributed asymptotically as a normal distribution with mean $\pi$ (in this case we take it equal to $0.6$) and variance $\frac{\pi\left(1-\pi\right)}{50}$.
```{r}
p <- 0.6
n <- 50

plot(function(x) dnorm(x, p, p*(1-p)/n))
abline(v = 18/n, col = "red")
```
Yes, it is very surprising.

### ex.4.5

b) We have $\hat{\pi} = \frac{938}{1447} = 0.65$, so we can use the square root of the Wald statistic with the estimated value of $\pi$  which is distributed as a student t distribution with n degrees of freedom (where $n=1447$). Considering a confidence interval of $1-\aplha = 0.95$, we have:
```{r}
n <- 1447
alpha <- 0.05
pi.h <- 0.65

Q <- qt(1-alpha/2, n)
left <- pi.h - sqrt(pi.h*(1-pi.h)/n) * Q
right <- pi.h + sqrt(pi.h*(1-pi.h)/n) * Q

left; right
```
We can then conclude with a pretty high confidence that the majorty of people is for legalization of marijuana.

### ex. 4.39

a) Conti semplici.

b) Conti semplici (in alternativa si può usare l'invarianza del maximum likelihood estimator).

c)
```{r}
# non funziona

y.bar <- 10
lambda.est <- 1 / y.bar

loglik <- function(lambda, n) {
  return(n*log(lambda) - lambda*n/lambda.est)
}

par(mfrow = c(2, 2))
plot(function(x) (loglik(x, 1) - loglik(lambda.est, 1)), xlim = c(0.01, 4))
plot(function(x) (loglik(x, 5) - loglik(lambda.est, 5)), xlim = c(0.01, 4))
plot(function(x) (loglik(x, 10) - loglik(lambda.est, 10)), xlim = c(0.01, 4))
```

### ex. 5.3

```{r}
n <- 116
pi.o <- 40 / n
pi.h <- 1/3

W = 2*n*log(pi.o*(1-pi.h)/(pi.h*(1-pi.o)))*pi.o + 2*n*log((1-pi.o)/(1-pi.h))
p.value <- pchisq(W, 1, lower.tail = FALSE)
p.value
```
```{r}
n <- 116
pi.o <- 40 / n
pi.h <- 1/3

p.value <- pnorm((pi.o-pi.h)/(sqrt(pi.h*(1-pi.h)/n)), lower.tail = FALSE)
p.value
```

### ex. 5.3

```{r}
# Wald test
n <- 3402
pi.o <- 0.73
pi.h <- 0.5

t <- (pi.o-pi.h) / sqrt(pi.h*(1-pi.h) / n)
t
p.value <- pnorm(t, lower.tail = FALSE) * 2
p.value
```
```{r}
# likelihood-ratio test
t <- 2*n*(pi.o*log(pi.o*(1-pi.h)/(pi.h*(1-pi.o))) + log((1-pi.o)/(1-pi.h)))
t
p.value <- pchisq(t, 1, lower.tail = FALSE)
p.value
```
I risultati ottenuti con i due test sono coerenti, anche se diversi (come atteso, aumentando n e assumendo un'ipotesi nulla più vicina alla realtà i due risultati convergono).

### ex. 5.19

```{r}
n.f <- 1178
n.m <- 945
pi.f <- 1017 / n.f
pi.m <- 703 / n.m

t <- (pi.f-pi.m) / sqrt(pi.f*(1-pi.f)/n.f + pi.m*(1-pi.m)/n.m)
t

p.value <- pnorm(t, lower.tail = FALSE)*2
p.value
```

```{r}
pi.c <- (703+1017) / (n.f+n.m)
t1 <- (pi.f-pi.m) / sqrt(pi.c*(1-pi.c)*(1/n.f+1/n.m))
t1

p.value1 <- pnorm(t1, lower.tail = FALSE)*2
p.value1
```

