{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## support vector machine con grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn import metrics\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from statistics import mean\n",
    "import numpy as np\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all'\n",
    "\n",
    "\n",
    "#setting up labels for dataset\n",
    "labels = ['class', 'spec_num', 'eccentr', 'asp_ratio', 'elong', 'solidity', 'stoch_conv', 'iso_factor', 'max_ind_depth', 'lobedness', 'av_intensity', 'av_contr', 'smooth', 'third_mom', 'unif', 'entropy']\n",
    "#importing data\n",
    "df = pd.read_csv(r'./leaf/leaf.csv', header = None, names = labels)\n",
    "# shuffling the dataframe\n",
    "df = df.sample(frac=1).reset_index()\n",
    "df = df.iloc[:, 1:17]   # needed to eliminate the old indexes column\n",
    "#display(df)\n",
    "#separating y from x\n",
    "X = df.iloc[:, 2:16]\n",
    "y = df.iloc[:, 0]\n",
    "\n",
    "#display(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### prima grid search (più grossolana) con k-fold CV e LOOCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 5\n",
    "\n",
    "# 'preparazione' indici di effectivness\n",
    "# si usa la funzione make_scorer per costruire le metriche che ci servono\n",
    "b_accuracy = metrics.make_scorer(metrics.balanced_accuracy_score)\n",
    "recall = metrics.make_scorer(metrics.recall_score, average='weighted')\n",
    "scorers = {'balanced_accuracy': b_accuracy, 'recall': recall}\n",
    "\n",
    "# building the range of the regularization parameter (C)\n",
    "exp = np.arange(-10, 12)\n",
    "reg_param = 10.**exp\n",
    "\n",
    "grid_param = {'C': reg_param,\n",
    "              'kernel': ['linear', 'poly', 'rbf', 'sigmoid'], \n",
    "              'degree': np.arange(2, 5),\n",
    "              'decision_function_shape': ['ovo', 'ovr']}\n",
    "\n",
    "clf_cv = GridSearchCV(svm.SVC(), grid_param, cv=k, scoring=scorers, refit=False)\n",
    "clf_cv.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# showing the mean values of effectivness indexes\n",
    "\n",
    "results_cv = pd.DataFrame(clf_cv.cv_results_)\n",
    "\n",
    "display(results_cv.loc[:, ('params', 'mean_test_balanced_accuracy', 'rank_test_balanced_accuracy', 'mean_test_recall', 'rank_test_recall')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "qua sotto visualizziamo prima soltanto le righe con i valori più alti della balanced accuracy (che corrispondono praticamente sempre a quelle con la recall più alta) e poi soltanto quelle con la roc auc ovo più alta (che corrispondono praticamente sempre a quelle con la roc auc ovr più alta).\n",
    "\n",
    "\n",
    "si hanno sei set di parametri che condividono la prima posizione perché:\n",
    "1) il kernel migliore è quello lineare, pertanto param_degree è ininfluente rispetto alla costruzione del modello\n",
    "2) per qualche motivo (da chiarire) decision_function_shape=ovo e decision_function_shape=ovr sono equivalenti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(results_cv[results_cv[\"rank_test_balanced_accuracy\"]==1])\n",
    "display(results_cv[results_cv[\"rank_test_balanced_accuracy\"]==1][\"mean_test_balanced_accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOOCV\n",
    "\n",
    "# building the range of the regularization parameter (C)\n",
    "exp = np.arange(-8, 8)\n",
    "reg_param = 10.**exp\n",
    "\n",
    "grid_param = {'C': reg_param,\n",
    "              'kernel': ['linear', 'poly', 'rbf', 'sigmoid'], \n",
    "              'degree': np.arange(2, 5),\n",
    "              'decision_function_shape': ['ovo', 'ovr']}\n",
    "\n",
    "clf_loocv = GridSearchCV(svm.SVC(), grid_param, cv=LeaveOneOut(), scoring='accuracy')\n",
    "clf_loocv.fit(X, y)\n",
    "print(clf_loocv.best_params_)\n",
    "print(clf_loocv.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "di seguito un modo alternativo di fare la grid search con cross validation nel quale si può anche calcolare ROC AUC\n",
    "\n",
    "l'unica differenza con sopra è che chiediamo a svc di calcolare anche la probabilità (necessaria al calcolo di roc_auc)\n",
    "\n",
    "è mooolto più lento: credo che il procedimento per il calcolo della probabilità sia abbstanza complicato, non come in tree (dove infatti è presente di default)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 5\n",
    "\n",
    "# 'preparazione' indici di effectivness\n",
    "# si usa la funzione make_scorer per costruire le metriche che ci servono\n",
    "auc_ovo = metrics.make_scorer(metrics.roc_auc_score, multi_class='ovo', needs_proba=True, average='weighted')\n",
    "auc_ovr = metrics.make_scorer(metrics.roc_auc_score, multi_class='ovr', needs_proba=True, average='weighted')\n",
    "scorers = {'roc_auc_ovo': auc_ovo, 'roc_auc_ovr': auc_ovr}\n",
    "\n",
    "# building the range of the regularization parameter (C)\n",
    "exp = np.arange(-8, 12)\n",
    "reg_param = 10.**exp\n",
    "\n",
    "grid_param = {'C': reg_param,\n",
    "              'kernel': ['linear', 'poly', 'rbf'], \n",
    "              'decision_function_shape': ['ovo', 'ovr']}\n",
    "\n",
    "clf_cv_2 = GridSearchCV(svm.SVC(probability=True), grid_param, cv=k, scoring=scorers, refit=False)\n",
    "clf_cv_2.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# showing the mean values of effectivness indexes\n",
    "\n",
    "results_cv_2 = pd.DataFrame(clf_cv_2.cv_results_)\n",
    "\n",
    "display(results_cv_2[results_cv_2[\"rank_test_roc_auc_ovo\"]==1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### seconda grid search (più specifica)\n",
    "\n",
    "visti i buoni risultati ottenuti, facciamo una ricerca più approfondita attorno ai valori di C migliori"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 5\n",
    "\n",
    "# 'preparazione' indici di effectivness\n",
    "# si usa la funzione make_scorer per costruire le metriche che ci servono\n",
    "b_accuracy = metrics.make_scorer(metrics.balanced_accuracy_score)\n",
    "auc_ovo = metrics.make_scorer(metrics.roc_auc_score, multi_class='ovo', needs_proba=True, average='weighted')\n",
    "scorers = {'balanced_accuracy': b_accuracy, 'roc_auc_ovo': auc_ovo}\n",
    "\n",
    "# building the range of the regularization parameter (C)\n",
    "exp = np.arange(2, 7, 0.2)\n",
    "reg_param = 10.**exp\n",
    "\n",
    "grid_param = {'C': reg_param}\n",
    "\n",
    "clf_cv_spec = GridSearchCV(svm.SVC(probability=True, kernel='linear', decision_function_shape='ovo'), grid_param, cv=k, scoring=scorers, refit=False)\n",
    "clf_cv_spec.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_cv_spec = pd.DataFrame(clf_cv_spec.cv_results_)\n",
    "\n",
    "display(results_cv_spec[results_cv_spec[\"rank_test_balanced_accuracy\"]==1])\n",
    "display(results_cv_spec[results_cv_spec[\"rank_test_balanced_accuracy\"]==1][\"mean_test_balanced_accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "l'accuracy migliora di poco rispetto alla grid search più generica"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
